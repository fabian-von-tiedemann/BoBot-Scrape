---
phase: 02-page-discovery
plan: 01
type: execute
depends_on: []
files_modified: [scrape.py]
---

<objective>
Extract all category links from the Botkyrka intranet rutiner page.

Purpose: Discover the structure of the page and identify all category links that lead to PDF documents.
Output: Modified scrape.py that extracts and prints all category links found on the page.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-plan.md
./summary.md
./.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-setup/01-02-SUMMARY.md

# Key files:
@scrape.py

**Tech stack available:** playwright, sync_playwright API
**Established patterns:** CDP connection via connect_over_cdp, contexts[0].pages[0], wait_for_load_state("networkidle")
**Constraining decisions:**
- Phase 01-02: Use sync_playwright API for simplicity
- Phase 01-02: Reuse existing browser tab instead of creating new one
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract category links from the page</name>
  <files>scrape.py</files>
  <action>
After navigation and page load, use Playwright to extract all category links from the page.

1. First inspect the page structure: use `page.content()` or developer tools to understand how links are organized
2. The page likely contains a list of links (possibly in a `<ul>` or similar container) pointing to category pages
3. Use `page.query_selector_all("a")` to get all links, then filter for:
   - Links that point to subpages (same domain, different path)
   - Links that appear to be category/navigation links (not external, not PDF direct links yet)
4. For each matching link, extract:
   - `href` attribute (the URL)
   - `text_content()` (the link text/category name)
5. Print the extracted links in a clear format

**What to avoid:**
- Don't use JavaScript evaluation when Playwright selectors work (avoid page.evaluate unless necessary)
- Don't hardcode specific selectors until we know the page structure - start broad then narrow
- Don't follow the links yet - just extract them for now
  </action>
  <verify>
Run: `.venv/bin/python scrape.py`
Output shows:
- Connection successful message
- List of extracted links with their URLs and text
- Count of total links found
  </verify>
  <done>Script extracts and prints all category links from the page. Links include URL and descriptive text.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Link extraction from the Botkyrka intranet rutiner page</what-built>
  <how-to-verify>
    1. Start Chrome with remote debugging (if not already running):
       `/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug`
    2. Log in to Botkyrka intranet if needed
    3. Run: `.venv/bin/python scrape.py`
    4. Verify in the output:
       - Links are being extracted (count > 0)
       - Link URLs look like category pages (not external sites)
       - Link text matches what you see on the actual page
    5. Compare extracted links with the actual page in Chrome - are all categories captured?
    6. Note: If the page structure is different than expected, describe what you see
  </how-to-verify>
  <resume-signal>Type "approved" if links look correct, or describe the page structure if extraction needs adjustment</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Script runs without errors
- [ ] Links are extracted and printed
- [ ] Link count and content verified by human
- [ ] No hardcoded assumptions that might break on different page structures
</verification>

<success_criteria>

- All tasks completed
- Script extracts category links from the page
- Human verified the extracted links match the actual page
- Ready for Phase 3 (PDF extraction from category pages)
</success_criteria>

<output>
After completion, create `.planning/phases/02-page-discovery/02-01-SUMMARY.md`:

# Phase 2 Plan 01: Page Discovery Summary

**[One-liner about what was discovered/built]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `scrape.py` - Added link extraction

## Page Structure Discovered

[Document what was learned about the page structure - this informs Phase 3]

## Decisions Made

[Any decisions about selectors, filtering, etc.]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

[Ready for Phase 3: PDF Extraction]
</output>
