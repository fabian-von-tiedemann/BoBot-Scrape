---
phase: 24-pipeline-runner
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [pipeline.py]
autonomous: true
---

<objective>
Create a unified pipeline CLI that orchestrates all ETL stages (scrape, convert, index, prompts) with timestamped output directories.

Purpose: Replace manual invocation of 5 separate scripts with a single `python pipeline.py` command that produces a versioned output structure.
Output: New `pipeline.py` CLI that creates `runs/YYYY-MM-DD-HHMM/` directories containing download, converted, indexes, and prompts subdirectories.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing scripts to orchestrate:
@scrape.py
@convert.py
@index_kb.py
@generate_prompts.py
@combine_prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pipeline.py CLI with timestamped run directories</name>
  <files>pipeline.py</files>
  <action>
Create a new pipeline.py CLI that:

1. **CLI arguments:**
   - `--run-dir` optional override (default: auto-generated timestamp)
   - `--skip-scrape` to skip download stage (use existing downloads/)
   - `--skip-ai` passed through to convert.py
   - `--force` to reprocess everything (passed to scrape.py and convert.py)

2. **Run directory structure:**
   Create `runs/YYYY-MM-DD-HHMM/` (e.g., `runs/2026-01-16-1430/`) with subdirectories:
   - `downloads/` - raw PDF/Word files
   - `converted/` - markdown files with AI metadata
   - `indexes/` - verksamhet index files
   - `prompts/` - generated prompts and combined prompts

3. **Stage orchestration:**
   Execute stages in sequence, passing appropriate paths:

   Stage 1 (optional): scrape.py
   - Skip if --skip-scrape flag
   - Output to: {run_dir}/downloads/
   - CSV goes to: {run_dir}/downloads/documents.csv

   Stage 2: convert.py
   - Input: {run_dir}/downloads/ (or downloads/ if --skip-scrape)
   - Output: {run_dir}/converted/
   - Pass through --skip-ai and --force flags

   Stage 3: index_kb.py
   - Input: {run_dir}/converted/
   - Output: {run_dir}/indexes/

   Stage 4: generate_prompts.py
   - Input: {run_dir}/indexes/
   - Output: {run_dir}/prompts/

   Stage 5: combine_prompts.py
   - Input: {run_dir}/prompts/
   - General: prompts/GENERAL.md (from root, not run-specific)
   - Output: {run_dir}/prompts/combined/

4. **Implementation approach:**
   Use subprocess.run() to invoke each script with appropriate arguments.
   Print stage banners and timing for each stage.
   Capture and display total run time.

5. **Error handling:**
   If any stage fails (non-zero exit), stop pipeline and report which stage failed.
   Do NOT use overly complex retry logic - just fail fast.

Pattern: Follow existing scripts' style (argparse, clear docstring, formatter_class=RawDescriptionHelpFormatter).
  </action>
  <verify>python pipeline.py --help shows usage with all options</verify>
  <done>pipeline.py CLI exists with --skip-scrape, --skip-ai, --force, --run-dir options</done>
</task>

<task type="auto">
  <name>Task 2: Test pipeline with --skip-scrape using existing downloads</name>
  <files>pipeline.py</files>
  <action>
Run the pipeline with --skip-scrape to validate all stages work correctly:

```bash
python pipeline.py --skip-scrape --skip-ai
```

This tests the full flow without Chrome dependency or API calls:
- Creates timestamped run directory
- convert.py processes downloads/ -> run_dir/converted/
- index_kb.py processes converted/ -> run_dir/indexes/
- generate_prompts.py processes indexes/ -> run_dir/prompts/
- combine_prompts.py creates run_dir/prompts/combined/

Expected output:
- New runs/YYYY-MM-DD-HHMM/ directory
- Subdirectories: converted/, indexes/, prompts/, prompts/combined/
- ~1149 markdown files in converted/
- 15 index files in indexes/
- 15 prompt files + combined/

If any issues, fix pipeline.py to handle path passing correctly.
  </action>
  <verify>
ls runs/ shows timestamped directory
ls runs/*/converted/ shows markdown files
ls runs/*/indexes/ shows 15 *-INDEX.md files
ls runs/*/prompts/combined/ shows 15 *-SYSTEM-PROMPT.md files
  </verify>
  <done>Pipeline completes all 4 stages successfully (when using --skip-scrape --skip-ai)</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python pipeline.py --help` shows all options
- [ ] `python pipeline.py --skip-scrape --skip-ai` completes all stages
- [ ] Run directory structure created with correct subdirectories
- [ ] All stage outputs land in run-specific subdirectories
</verification>

<success_criteria>
- pipeline.py CLI created with documented options
- Timestamped run directories work correctly
- All 5 stages execute in sequence with correct paths
- --skip-scrape allows running without Chrome
- Pipeline completes successfully with test run
</success_criteria>

<output>
After completion, create `.planning/phases/24-pipeline-runner/24-01-SUMMARY.md`
</output>
