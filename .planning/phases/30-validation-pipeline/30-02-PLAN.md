---
phase: 30-validation-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["30-01"]
files_modified:
  - src/qa/validator.py
  - generate_qa.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "CLI accepts --validate flag to run validation on qa/answers.yaml"
    - "Passed QA pairs written to qa/qa_passed.jsonl"
    - "Rejected QA pairs written to qa/qa_rejected.jsonl with failure reasoning"
    - "Summary statistics printed: total, passed, failed, pass rate, avg score"
  artifacts:
    - path: "src/qa/validator.py"
      provides: "validate_batch() for batch processing"
      exports: ["validate_batch"]
    - path: "generate_qa.py"
      provides: "CLI --validate mode"
      contains: "--validate"
    - path: "qa/qa_passed.jsonl"
      provides: "Validated QA pairs in JSONL format"
    - path: "qa/qa_rejected.jsonl"
      provides: "Rejected QA pairs with validation details"
  key_links:
    - from: "generate_qa.py"
      to: "src/qa/validator.py"
      via: "validate_batch import"
      pattern: "from src\\.qa import.*validate_batch"
    - from: "src/qa/validator.py"
      to: "qa/qa_passed.jsonl"
      via: "JSONL write"
      pattern: "write_jsonl|json\\.dumps"
---

<objective>
Wire validation into CLI with batch processing, separate output streams for passed/rejected pairs, and summary statistics.

Purpose: Enables running validation as part of the QA pipeline with clear output separation for review.
Output: CLI --validate mode producing qa_passed.jsonl and qa_rejected.jsonl.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/30-validation-pipeline/30-RESEARCH.md
@.planning/phases/30-validation-pipeline/30-CONTEXT.md
@.planning/phases/30-validation-pipeline/30-01-SUMMARY.md

# Existing code to follow patterns from
@src/qa/answer.py
@generate_qa.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validate_batch() with progress tracking</name>
  <files>src/qa/validator.py, src/qa/__init__.py</files>
  <action>
Add validate_batch() function to src/qa/validator.py following batch pattern from answer.py:

```python
def validate_batch(
    qa_pairs: list[dict],
    retriever: SwedishRetriever,
    doc_contents: dict[str, str],
    output_passed: Path,
    output_rejected: Path,
    threshold: float = 0.7,
) -> dict:
    """
    Validate batch of QA pairs with progress tracking.

    Args:
        qa_pairs: List of QA entry dicts from answers.yaml
        retriever: SwedishRetriever with loaded index
        doc_contents: Dict mapping document paths to content
        output_passed: Path for passed pairs JSONL
        output_rejected: Path for rejected pairs JSONL
        threshold: Composite score threshold (default 0.7)

    Returns:
        Summary dict with total, passed, rejected, pass_rate, avg_score
    """
```

Implementation:
1. Use rich.progress (same pattern as answer.py)
2. For each QA pair, call validate_qa_pair()
3. Separate into passed/rejected lists based on result.passed
4. Write to JSONL files (one JSON object per line)
5. Return summary statistics

Add helper function for JSONL writing:
```python
def write_jsonl(path: Path, entries: list[dict]) -> None:
    """Write entries to JSONL file (one JSON object per line)."""
    with open(path, 'w', encoding='utf-8') as f:
        for entry in entries:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
```

Add load_document_contents() helper:
```python
def load_document_contents(docs_dir: Path) -> dict[str, str]:
    """Load all document contents for LLM judge context."""
    doc_contents = {}
    for md_file in docs_dir.rglob("*.md"):
        # Use relative path matching citation format
        rel_path = f"{md_file.parent.name}/{md_file.name}"
        doc_contents[rel_path] = md_file.read_text(encoding='utf-8')
    return doc_contents
```

Update src/qa/__init__.py to export: validate_batch, load_document_contents
  </action>
  <verify>python -c "from src.qa import validate_batch, load_document_contents; print('Batch exports OK')"</verify>
  <done>
- validate_batch() processes QA pairs with progress bar
- write_jsonl() writes JSONL format output
- load_document_contents() loads documents for LLM context
- Exports added to __init__.py
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire CLI --validate mode</name>
  <files>generate_qa.py</files>
  <action>
Extend generate_qa.py with --validate mode following existing CLI pattern:

1. Add argument:
```python
parser.add_argument(
    '--validate',
    action='store_true',
    help='Validate QA pairs from answers.yaml, output to qa_passed.jsonl and qa_rejected.jsonl'
)
```

2. Add validation execution block after --answers handling:
```python
if args.validate:
    logger.info("Running validation on QA pairs...")

    # Load answers
    answers_path = args.output / "answers.yaml"
    if not answers_path.exists():
        logger.error(f"answers.yaml not found at {answers_path}. Run with --answers first.")
        sys.exit(1)

    with open(answers_path, encoding='utf-8') as f:
        answers_data = yaml.safe_load(f)

    # Flatten categories to list
    qa_pairs = []
    for category, entries in answers_data.get('categories', {}).items():
        qa_pairs.extend(entries)

    # Load retriever and documents
    from src.qa import SwedishRetriever, validate_batch, load_document_contents

    index_dir = args.output / "embeddings"
    if not (index_dir / "chunks.index").exists():
        logger.error(f"Index not found. Run with --build-index first.")
        sys.exit(1)

    retriever = SwedishRetriever(index_dir)
    retriever.load_index()

    doc_contents = load_document_contents(Path(args.input))

    # Run validation
    stats = validate_batch(
        qa_pairs=qa_pairs,
        retriever=retriever,
        doc_contents=doc_contents,
        output_passed=args.output / "qa_passed.jsonl",
        output_rejected=args.output / "qa_rejected.jsonl",
        threshold=0.7
    )

    # Print summary
    print(f"\n=== Validation Summary ===")
    print(f"Total pairs: {stats['total']}")
    print(f"Passed: {stats['passed']} ({stats['pass_rate']:.1%})")
    print(f"Rejected: {stats['rejected']}")
    print(f"Average score (passed): {stats['avg_score']:.2f}")
    print(f"\nOutput files:")
    print(f"  Passed: {args.output / 'qa_passed.jsonl'}")
    print(f"  Rejected: {args.output / 'qa_rejected.jsonl'}")
```

3. Update help text in main to mention validation step
  </action>
  <verify>python generate_qa.py --help | grep -q validate && echo "CLI --validate OK"</verify>
  <done>
- CLI accepts --validate flag
- Loads answers.yaml and flattens categories
- Calls validate_batch with proper paths
- Prints summary statistics
  </done>
</task>

<task type="auto">
  <name>Task 3: Validate end-to-end with test batch</name>
  <files>-</files>
  <action>
Run validation on existing qa/answers.yaml (10 QA pairs from Phase 29):

```bash
# Ensure GEMINI_API_KEY is set
export GEMINI_API_KEY="${GEMINI_API_KEY}"

# Run validation
python generate_qa.py --input converted --output qa --validate
```

Expected results:
1. Progress bar shows validation of 10 pairs
2. qa/qa_passed.jsonl created with passing pairs
3. qa/qa_rejected.jsonl created (may be empty if all pass)
4. Summary statistics printed
5. Each passed entry has validation object with:
   - composite_score >= 0.7
   - source_verification.similarity_score
   - quality_assessment.relevans, korrekthet, fullstandighet

Verify output format:
```bash
# Check JSONL format (one object per line)
head -1 qa/qa_passed.jsonl | python -c "import json,sys; d=json.load(sys.stdin); print(d.keys())"

# Check validation fields present
head -1 qa/qa_passed.jsonl | python -c "import json,sys; d=json.load(sys.stdin); print(d.get('validation',{}).keys())"
```
  </action>
  <verify>
# Verify files exist and have content
test -f qa/qa_passed.jsonl && echo "qa_passed.jsonl exists"
test -f qa/qa_rejected.jsonl && echo "qa_rejected.jsonl exists"
head -1 qa/qa_passed.jsonl | python -c "import json,sys; d=json.load(sys.stdin); assert 'validation' in d; print('Validation field OK')"
  </verify>
  <done>
- Validation runs successfully on 10 QA pairs
- JSONL output files created with proper format
- Each entry contains validation object with scores and reasoning
- Summary statistics accurate
  </done>
</task>

</tasks>

<verification>
Run complete validation pipeline:
```bash
# Full validation run
python generate_qa.py --input converted --output qa --validate

# Verify output files
ls -la qa/qa_passed.jsonl qa/qa_rejected.jsonl

# Verify JSONL format
wc -l qa/qa_passed.jsonl qa/qa_rejected.jsonl

# Verify validation fields
python -c "
import json
with open('qa/qa_passed.jsonl') as f:
    for line in f:
        entry = json.loads(line)
        v = entry['validation']
        assert 'composite_score' in v
        assert 'source_verification' in v
        assert v.get('quality_assessment') or v['source_verification']['similarity_score'] < 0.5
        print(f'Score: {v[\"composite_score\"]:.2f}')
"
```
</verification>

<success_criteria>
- CLI --validate mode works end-to-end
- qa/qa_passed.jsonl contains validated pairs with scores
- qa/qa_rejected.jsonl contains rejected pairs with failure reasoning
- Summary shows: total, passed, rejected, pass rate, average score
- JSONL format is valid (one JSON object per line)
- Each entry has complete validation object
</success_criteria>

<output>
After completion, create `.planning/phases/30-validation-pipeline/30-02-SUMMARY.md`
</output>
