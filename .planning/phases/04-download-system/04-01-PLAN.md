---
phase: 04-download-system
plan: 01
type: execute
depends_on: []
files_modified: [scrape.py]
---

<objective>
Download all extracted documents (PDFs and Word files) to organized folder structure.

Purpose: Complete the scraper by actually downloading all 1195 documents found in Phase 3.
Output: `downloads/` folder with subfolders per category, all documents downloaded.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-plan.md
./summary.md
./.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-pdf-extraction/03-01-SUMMARY.md
@scrape.py

**From Phase 3:**
- 1195 documents extracted (1067 PDFs + 128 Word files)
- `documents_by_category` dict: {category_name: [{"url": url, "type": "pdf"|"doc"|"docx"}]}
- DOCUMENT_EXTENSIONS constant already includes .pdf, .doc, .docx

**User request:**
- Download both PDFs and Word files to same folder per category
- Folder structure: `downloads/{category_name}/` containing all documents
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add download functionality to scrape.py</name>
  <files>scrape.py</files>
  <action>
After document extraction loop, add download logic:

1. Create `downloads/` base directory (use `os.makedirs` with `exist_ok=True`)
2. For each category in `documents_by_category`:
   - Create category subfolder: `downloads/{category_name}/`
   - Sanitize folder name (replace invalid chars like `/` with `-`)
3. For each document in category:
   - Extract filename from URL (last path segment)
   - Download using `page.context.request.get(url)` to get bytes
   - Write to `downloads/{category_name}/{filename}`
   - Print progress: "Downloaded {filename} to {category}/"
4. Print final summary: total downloaded, any failures

Import `os` at top of file.
Use `page.context.request.get()` for download (Playwright's built-in HTTP client, shares session cookies).
Handle download errors gracefully - skip failed downloads and report at end.
  </action>
  <verify>.venv/bin/python scrape.py runs and creates downloads/ folder with subfolders and files</verify>
  <done>Downloads folder created with 15 category subfolders, documents downloaded to appropriate folders</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Document download system that saves all PDFs and Word files to organized folders</what-built>
  <how-to-verify>
    1. Run: .venv/bin/python scrape.py
    2. Wait for script to complete (will take several minutes for 1195 files)
    3. Check: ls downloads/
    4. Verify: 15 category folders exist
    5. Check one folder: ls "downloads/Hemtjanst/"
    6. Verify: Contains .pdf and possibly .doc/.docx files
    7. Open a PDF to confirm it downloaded correctly (not corrupted)
  </how-to-verify>
  <resume-signal>Type "approved" if files downloaded correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `downloads/` directory exists
- [ ] 15 category subfolders created
- [ ] Documents downloaded to correct folders
- [ ] No Python errors during execution
- [ ] At least one downloaded file opens correctly
</verification>

<success_criteria>

- All tasks completed
- Downloads folder structure matches categories
- Documents (PDFs and Word files) saved to correct category folders
- Download summary shows count and any failures
- Phase 4 complete - full scraper functional
  </success_criteria>

<output>
After completion, create `.planning/phases/04-download-system/04-01-SUMMARY.md`:

# Phase 4 Plan 01: Download System Summary

**[Downloaded X documents to organized category folders]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `scrape.py` - Added download functionality
- `downloads/` - Created folder structure with downloaded files

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Phase complete - BoBot-Scrape fully functional
</output>
