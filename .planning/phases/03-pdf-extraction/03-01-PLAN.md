---
phase: 03-pdf-extraction
plan: 01
type: execute
depends_on: []
files_modified: [scrape.py]
---

<objective>
Follow each category link and extract all document URLs (PDF + Word) from category pages.

Purpose: Build the complete list of document URLs to download, organized by category.
Output: Script that traverses all 15 categories and collects PDF and Word document links.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/MILESTONE-CONTEXT.md
@.planning/phases/02-page-discovery/02-01-SUMMARY.md

@scrape.py

**Milestone update (from MILESTONE-CONTEXT.md):**
- Phase 3 extended to extract BOTH PDF (.pdf) and Word (.doc, .docx) files
- Track all document URLs for CSV logging in Phase 4

**Tech stack available:**
- sync_playwright API with CDP connection
- RUTINER_CATEGORIES constant (15 categories)
- Exact-match filtering pattern

**Patterns established:**
- CDP connection: `connect_over_cdp → contexts[0] → pages[0]`
- Wait for `networkidle` before extraction
- Normalize relative URLs to absolute

**Current state:**
Script extracts 15 category links. Next: visit each and extract document links.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Visit each category and extract document links (PDF + Word)</name>
  <files>scrape.py</files>
  <action>
After extracting category_links, loop through each category:
1. Navigate to category URL with page.goto()
2. Wait for networkidle
3. Extract all links using page.query_selector_all("a")
4. Filter for document links where href ends with:
   - ".pdf" (PDF files)
   - ".doc" (Word 97-2003)
   - ".docx" (Word 2007+)
   Use case-insensitive matching (.lower())
5. Normalize relative URLs to absolute using base_domain
6. Store as dict: {category_name: [{"url": url, "type": "pdf"|"doc"|"docx"}]}
7. Print progress: "Processing category X/15: {name}"
8. Print summary per category: "  Found Y documents (Z PDFs, W Word files)"
9. Print total summary: "Total: X documents across all categories"

Define DOCUMENT_EXTENSIONS = (".pdf", ".doc", ".docx") as constant for clarity.

AVOID: Don't use href.endswith() without .lower() — extensions may vary in case.
AVOID: Don't break on errors — skip failed categories and report at end.
  </action>
  <verify>.venv/bin/python scrape.py runs and shows document count per category</verify>
  <done>Script visits all 15 categories and reports total document count (PDFs + Word)</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Document link extraction (PDF + Word) from all 15 categories</what-built>
  <how-to-verify>
    1. Start Chrome with debug flags if not running:
       /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug
    2. Log in to Botkyrka intranet if needed
    3. Run: .venv/bin/python scrape.py
    4. Confirm:
       - Script visits all 15 categories
       - Reports document count per category (shows PDFs and Word files separately)
       - Shows total document count
       - No errors or missing categories
       - Word files detected if present on any category page
  </how-to-verify>
  <resume-signal>Type "approved" if documents found correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Script visits all 15 category pages
- [ ] PDF links extracted (filtered by .pdf extension)
- [ ] Word links extracted (filtered by .doc/.docx extensions)
- [ ] Total document count reported (broken down by type)
- [ ] No crashes or unhandled errors
</verification>

<success_criteria>

- All 15 categories visited
- PDF and Word links extracted and stored by category
- Script runs without errors
- Human verified output looks correct
</success_criteria>

<output>
After completion, create `.planning/phases/03-pdf-extraction/03-01-SUMMARY.md`
</output>
