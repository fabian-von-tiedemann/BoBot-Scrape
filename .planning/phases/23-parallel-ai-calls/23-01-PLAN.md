---
phase: 23-parallel-ai-calls
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/ai/gemini.py, src/ai/__init__.py, convert.py]
autonomous: true
---

<objective>
Implement parallel Gemini API calls for faster document metadata generation.

Purpose: Reduce conversion time from sequential (~115+ seconds in API delays alone for 1149 docs) to parallel batch processing.
Output: Updated convert.py with batch_generate_metadata() using ThreadPoolExecutor.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/ai/gemini.py
@src/ai/__init__.py
@convert.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add batch_generate_metadata function to AI module</name>
  <files>src/ai/gemini.py, src/ai/__init__.py</files>
  <action>
Create batch_generate_metadata() function in src/ai/gemini.py:

1. Add function signature:
   ```python
   def batch_generate_metadata(
       texts: list[str],
       max_workers: int = 10,
       delay: float = 0.05
   ) -> list[Optional[DocumentMetadata]]:
   ```

2. Use concurrent.futures.ThreadPoolExecutor for parallel execution:
   - max_workers controls parallelism (default 10 is conservative for API rate limits)
   - Map texts to generate_metadata() calls
   - Preserve order of results matching input texts

3. Import ThreadPoolExecutor from concurrent.futures at top of file

4. Export batch_generate_metadata from src/ai/__init__.py

Note: Keep existing generate_metadata() unchanged for backwards compatibility. The batch function simply wraps multiple calls in parallel.
  </action>
  <verify>python -c "from src.ai import batch_generate_metadata; print('OK')"</verify>
  <done>batch_generate_metadata() function exists and is importable</done>
</task>

<task type="auto">
  <name>Task 2: Integrate batch processing in convert.py</name>
  <files>convert.py</files>
  <action>
Modify convert.py to use batch AI processing:

1. Add --batch-size CLI argument (default: 50):
   ```python
   parser.add_argument(
       "--batch-size",
       type=int,
       default=50,
       help="Number of documents to process in parallel batches (default: 50)"
   )
   ```

2. Refactor main() processing loop:

   a. First pass: Extract text from all documents that need processing
      - Collect (doc_path, output_path, text) tuples for docs that need conversion
      - Print "Extracting text from N documents..."

   b. Second pass: Batch AI metadata generation (unless --skip-ai)
      - Call batch_generate_metadata() with all texts
      - Print "Generating AI metadata for N documents (batch size: {batch_size})..."

   c. Third pass: Write markdown files with metadata
      - Match metadata results back to documents by index
      - Write files with frontmatter

3. Keep existing --skip-ai behavior working (skip the batch AI step entirely)

4. Update progress output to show batch progress

Design note: This changes from "process each file end-to-end" to "extract all → AI batch → write all". This maximizes API parallelism while keeping file I/O simple.
  </action>
  <verify>.venv/bin/python convert.py --help | grep batch-size</verify>
  <done>--batch-size argument visible in help, convert.py uses batch processing</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] batch_generate_metadata() importable from src.ai
- [ ] convert.py --help shows --batch-size argument
- [ ] .venv/bin/python convert.py --skip-ai completes (no AI, test basic flow)
- [ ] Code has no syntax errors (python -m py_compile convert.py)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Batch processing architecture in place
- Backwards compatible (--skip-ai still works, single file still works)
</success_criteria>

<output>
After completion, create `.planning/phases/23-parallel-ai-calls/23-01-SUMMARY.md`
</output>
