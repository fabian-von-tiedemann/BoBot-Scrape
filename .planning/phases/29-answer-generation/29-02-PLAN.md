---
phase: 29-answer-generation
plan: 02
type: execute
wave: 2
depends_on: [29-01]
files_modified:
  - src/qa/answer.py
  - src/qa/__init__.py
  - generate_qa.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Each answer is grounded in retrieved source document content"
    - "Answers include inline citations [source:document.md#section]"
    - "Answer text uses klarsprak (B1 Swedish, max 15 words per sentence, active voice)"
    - "CLI can generate answers for questions from Phase 28"
  artifacts:
    - path: "src/qa/answer.py"
      provides: "Answer generation with citations and klarsprak"
      exports: ["Citation", "GeneratedAnswer", "QAEntry", "generate_answer", "generate_answers_batch"]
    - path: "generate_qa.py"
      provides: "CLI with --answers mode"
      contains: "--answers"
    - path: "qa/answers.yaml"
      provides: "Generated QA pairs with citations"
  key_links:
    - from: "src/qa/answer.py"
      to: "src/qa/retriever.py"
      via: "SwedishRetriever.retrieve()"
      pattern: "retriever\\.retrieve"
    - from: "src/qa/answer.py"
      to: "gemini-2.0-flash"
      via: "genai.Client structured output"
      pattern: "gemini-2\\.0-flash"
    - from: "generate_qa.py"
      to: "src/qa/answer.py"
      via: "answer generation imports"
      pattern: "from src\\.qa import.*generate_answer"
---

<objective>
Generate grounded answers with explicit source citations in klarsprak Swedish.

Purpose: Transform questions from Phase 28 into validated QA pairs anchored in source documents.
Output: Answer generation module and CLI integration producing qa/answers.yaml.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-answer-generation/29-RESEARCH.md
@.planning/phases/29-answer-generation/29-CONTEXT.md
@.planning/phases/29-answer-generation/29-01-SUMMARY.md
@src/qa/question.py
@src/qa/__init__.py
@generate_qa.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create answer generation module</name>
  <files>src/qa/answer.py, src/qa/__init__.py</files>
  <action>
    Create src/qa/answer.py with Pydantic models and generation logic:

    Citation model:
    - document: str (full path, e.g., rutiner/handtvatt.md)
    - section: str (section heading if available, empty otherwise)

    GeneratedAnswer model (for Gemini structured output):
    - answer: str (klarsprak Swedish, B1 level)
    - citations: list[Citation] (min_length=1)
    - coverage: Literal["full", "partial", "none"]
    - confidence: float (0.0-1.0)

    QAEntry model (full output entry):
    - question: str
    - answer: str
    - citations: list[dict]
    - coverage: str
    - confidence: float
    - source_document: str
    - section: str
    - question_type: str
    - persona: dict
    - category: str
    - generated_at: str

    ANSWER_GENERATION_PROMPT constant:
    - Klarsprak requirements: max 15 words per sentence, active voice ("Du tvattar" not "ska tvattas"), use "du"
    - Citation format: [source:document.md#section] inline
    - Short answers: 1-3 sentences
    - Quote only for critical exact wording
    - Handle partial coverage: note what's not covered

    generate_answer(question: str, retrieved_chunks: list[dict], delay: float = 0.2) -> GeneratedAnswer | None:
    - Formats sources for prompt (with source reference per chunk)
    - Calls Gemini 2.0 Flash with structured output (response_schema=GeneratedAnswer)
    - Returns parsed response or None on failure

    create_qa_entry(question_entry: dict, answer: GeneratedAnswer) -> QAEntry:
    - Combines Phase 28 question with generated answer
    - Preserves all original metadata (persona, source_document, etc.)

    generate_answers_batch(questions: list[dict], retriever, output_path: Path, max_workers: int = 5, delay: float = 0.2) -> None:
    - Processes questions with ThreadPoolExecutor
    - For each question: retrieve(question, top_k=5), generate_answer()
    - Creates QAEntry for successful generations
    - Groups by category (same as questions.yaml structure)
    - Writes to YAML with generated_at, total_qa_pairs, categories

    Update src/qa/__init__.py to export answer module components.

    Use Patterns 3 and 4 from RESEARCH.md as reference.
  </action>
  <verify>
    python -c "from src.qa.answer import Citation, GeneratedAnswer, QAEntry, generate_answer, create_qa_entry, generate_answers_batch; print('OK')"
  </verify>
  <done>
    - Citation and GeneratedAnswer Pydantic models exist
    - QAEntry combines question and answer metadata
    - ANSWER_GENERATION_PROMPT enforces klarsprak and citations
    - generate_answer() calls Gemini with structured output
    - generate_answers_batch() processes questions with retrieval
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire CLI for answer generation</name>
  <files>generate_qa.py</files>
  <action>
    Extend generate_qa.py CLI with answer generation mode:

    Add new arguments:
    - --answers: Flag to run answer generation (requires questions.yaml to exist)
    - --build-index: Flag to build/rebuild FAISS index from converted/ documents
    - --questions-file: Path to questions YAML (default: qa/questions.yaml)
    - --index-dir: Path to embeddings index directory (default: qa/embeddings)

    Add build_index_command() function:
    - Uses chunk_all_documents(args.input) to chunk converted/ documents
    - Creates SwedishRetriever(args.index_dir)
    - Calls retriever.build_index(chunks)
    - Reports chunk count and index location

    Add generate_answers_command() function:
    - Loads questions from questions.yaml
    - Creates/loads SwedishRetriever
    - If index doesn't exist, prompts to run --build-index first
    - Calls generate_answers_batch()
    - Reports total QA pairs generated

    Update main() logic:
    - If --build-index: run build_index_command() and exit
    - If --answers: run generate_answers_command() and exit
    - Otherwise: existing question generation flow

    CLI usage examples (add to epilog):
      %(prog)s --build-index              Build FAISS index from documents
      %(prog)s --answers                  Generate answers for questions.yaml
      %(prog)s --answers --limit 10       Generate answers for first 10 questions
  </action>
  <verify>
    python generate_qa.py --help (shows --answers, --build-index options)
    python generate_qa.py --build-index (builds index in qa/embeddings/)
    python generate_qa.py --answers --limit 5 (generates 5 QA pairs)
  </verify>
  <done>
    - --build-index creates FAISS index from converted/ documents
    - --answers generates answers using retriever and Gemini
    - qa/answers.yaml contains QA pairs with citations
    - CLI provides clear progress feedback
  </done>
</task>

<task type="auto">
  <name>Task 3: Validate end-to-end with sample run</name>
  <files>qa/answers.yaml</files>
  <action>
    Run end-to-end test:

    1. Build index (if not already done):
       python generate_qa.py --build-index

    2. Generate answers for small batch:
       python generate_qa.py --answers --limit 10 --verbose

    3. Validate output qa/answers.yaml:
       - Has generated_at timestamp
       - Has total_qa_pairs count
       - Has categories dict with QA entries
       - Each entry has: question, answer, citations, coverage, confidence, source_document, persona, category

    4. Spot-check quality:
       - Answers are in Swedish
       - Citations reference real documents from converted/
       - Sentences are reasonably short (klarsprak)
       - Active voice used ("Du" form)

    Log any issues for future validation phase (Phase 30).
  </action>
  <verify>
    cat qa/answers.yaml | head -50 (shows valid YAML structure)
    python -c "import yaml; d=yaml.safe_load(open('qa/answers.yaml')); print(f'Total: {d[\"total_qa_pairs\"]} QA pairs')"
  </verify>
  <done>
    - qa/answers.yaml exists with valid structure
    - QA pairs have questions, answers, and citations
    - Answers are grounded in source documents
    - CLI workflow is complete and documented
  </done>
</task>

</tasks>

<verification>
Full pipeline test:
```bash
# 1. Ensure index exists
python generate_qa.py --build-index

# 2. Generate answers
python generate_qa.py --answers --limit 20 --verbose

# 3. Validate output
python -c "
import yaml
from pathlib import Path

with open('qa/answers.yaml') as f:
    data = yaml.safe_load(f)

print(f'Generated at: {data[\"generated_at\"]}')
print(f'Total QA pairs: {data[\"total_qa_pairs\"]}')
print(f'Categories: {list(data[\"categories\"].keys())}')

# Check first entry
for cat, entries in data['categories'].items():
    if entries:
        e = entries[0]
        print(f'\\nSample from {cat}:')
        print(f'  Q: {e[\"question\"][:80]}...')
        print(f'  A: {e[\"answer\"][:80]}...')
        print(f'  Citations: {len(e[\"citations\"])}')
        print(f'  Coverage: {e[\"coverage\"]}')
        break
"
```
</verification>

<success_criteria>
- Answer generation produces grounded responses with citations
- Citations use format [source:document.md#section]
- Answers follow klarsprak (B1 Swedish, short sentences, active voice)
- CLI --answers mode works with progress tracking
- qa/answers.yaml contains valid QA pairs grouped by category
- Requirements AGEN-01 through AGEN-04 are satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/29-answer-generation/29-02-SUMMARY.md`
</output>
