---
phase: 29-answer-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/qa/chunker.py
  - src/qa/retriever.py
  - src/qa/__init__.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Documents can be split into 512-token chunks with overlap"
    - "Swedish text can be embedded using KBLab SBERT model"
    - "FAISS index stores embeddings for similarity search"
    - "Questions can retrieve top-5 relevant document chunks"
  artifacts:
    - path: "src/qa/chunker.py"
      provides: "Document chunking with section extraction"
      exports: ["DocumentChunk", "chunk_document", "chunk_all_documents"]
    - path: "src/qa/retriever.py"
      provides: "Swedish semantic retrieval with FAISS"
      exports: ["SwedishRetriever"]
    - path: "requirements.txt"
      provides: "New dependencies for embeddings"
      contains: "sentence-transformers"
  key_links:
    - from: "src/qa/retriever.py"
      to: "KBLab/sentence-bert-swedish-cased"
      via: "SentenceTransformer model loading"
      pattern: "SentenceTransformer.*KBLab"
    - from: "src/qa/retriever.py"
      to: "faiss"
      via: "IndexFlatIP creation and search"
      pattern: "faiss\\.(IndexFlatIP|write_index|read_index)"
---

<objective>
Build semantic retrieval infrastructure for answer generation.

Purpose: Enable finding relevant document passages to ground answers in source content.
Output: Chunking utilities and Swedish FAISS retriever ready for answer generation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-answer-generation/29-RESEARCH.md
@.planning/phases/29-answer-generation/29-CONTEXT.md
@.planning/phases/28-question-generation/28-01-SUMMARY.md
@src/qa/question.py
@src/qa/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and create document chunker</name>
  <files>requirements.txt, src/qa/chunker.py</files>
  <action>
    1. Add new dependencies to requirements.txt:
       - sentence-transformers>=2.2.0
       - faiss-cpu>=1.7.0
       - tiktoken>=0.5.0

    2. Create src/qa/chunker.py with:

    DocumentChunk dataclass:
    - content: str (chunk text)
    - document_path: str (relative path: category/filename.md)
    - section: str (nearest markdown heading, empty if none)
    - chunk_index: int
    - token_count: int

    chunk_document() function:
    - Takes document_path: Path, chunk_size: int = 512, overlap: int = 128
    - Uses tiktoken cl100k_base encoding for accurate token counting
    - Skips YAML frontmatter (between --- markers)
    - Extracts section headings using regex (^#+\s+(.+)$)
    - Returns list[DocumentChunk]

    chunk_all_documents() function:
    - Takes documents_dir: Path
    - Finds all .md files recursively
    - Chunks each document
    - Returns flat list of all chunks with progress bar (Rich)

    Use Pattern 1 from RESEARCH.md as reference.
  </action>
  <verify>
    python -c "from src.qa.chunker import DocumentChunk, chunk_document, chunk_all_documents; print('OK')"
    Test manually: chunk_document on a sample converted/ document returns 1+ chunks
  </verify>
  <done>
    - DocumentChunk dataclass exists with required fields
    - chunk_document() splits documents into overlapping token chunks
    - chunk_all_documents() processes entire converted/ directory
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Swedish FAISS retriever</name>
  <files>src/qa/retriever.py, src/qa/__init__.py</files>
  <action>
    Create src/qa/retriever.py with SwedishRetriever class:

    __init__(self, index_dir: Path):
    - Loads KBLab/sentence-bert-swedish-cased model via SentenceTransformer
    - Stores index_dir for persistence
    - Initializes empty index and chunks_meta list

    build_index(self, chunks: list[DocumentChunk]) -> None:
    - Generates embeddings for all chunk contents (show_progress_bar=True)
    - Normalizes embeddings with faiss.normalize_L2 for cosine similarity
    - Creates IndexFlatIP (inner product = cosine after normalization)
    - Stores chunk metadata (content, document_path, section, chunk_index)
    - Saves index to index_dir/chunks.index via faiss.write_index
    - Saves metadata to index_dir/chunks_meta.json

    load_index(self) -> None:
    - Loads existing index from chunks.index
    - Loads metadata from chunks_meta.json

    retrieve(self, query: str, top_k: int = 5) -> list[dict]:
    - Embeds query text
    - Normalizes embedding
    - Searches index for top_k results
    - Returns list of dicts with content, document_path, section, score

    Update src/qa/__init__.py to export:
    - DocumentChunk, chunk_document, chunk_all_documents from chunker
    - SwedishRetriever from retriever

    Use Pattern 2 from RESEARCH.md as reference.
  </action>
  <verify>
    python -c "from src.qa import SwedishRetriever, chunk_all_documents; print('OK')"
    pip install sentence-transformers faiss-cpu tiktoken (if not already)
    Test: Build small index from 5 documents, verify retrieve() returns results
  </verify>
  <done>
    - SwedishRetriever class loads Swedish SBERT model
    - build_index() creates and persists FAISS index with metadata
    - load_index() restores from disk
    - retrieve() returns top-k chunks with similarity scores
    - All exports work from src.qa module
  </done>
</task>

</tasks>

<verification>
Run integration test:
```bash
# Install new dependencies
pip install sentence-transformers faiss-cpu tiktoken

# Test imports
python -c "
from src.qa import (
    DocumentChunk, chunk_document, chunk_all_documents,
    SwedishRetriever
)
print('All imports OK')
"

# Test chunking on sample document
python -c "
from pathlib import Path
from src.qa import chunk_document

# Find a sample document
docs = list(Path('converted').rglob('*.md'))[:1]
if docs:
    chunks = chunk_document(docs[0])
    print(f'Chunked {docs[0].name}: {len(chunks)} chunks')
    if chunks:
        print(f'First chunk: {chunks[0].token_count} tokens, section: {chunks[0].section or \"(none)\"}')
"

# Test retriever (minimal, model download may take time)
python -c "
from pathlib import Path
from src.qa import SwedishRetriever

retriever = SwedishRetriever(Path('qa/embeddings'))
print(f'Retriever initialized, model loaded')
"
```
</verification>

<success_criteria>
- requirements.txt includes sentence-transformers, faiss-cpu, tiktoken
- DocumentChunk, chunk_document, chunk_all_documents work correctly
- SwedishRetriever initializes with KBLab Swedish SBERT
- build_index creates persisted FAISS index
- retrieve() returns relevant chunks for queries
- All exports accessible from src.qa module
</success_criteria>

<output>
After completion, create `.planning/phases/29-answer-generation/29-01-SUMMARY.md`
</output>
