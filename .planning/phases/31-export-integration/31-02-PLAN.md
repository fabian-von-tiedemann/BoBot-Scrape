---
phase: 31-export-integration
plan: 02
type: execute
wave: 2
depends_on: ["31-01"]
files_modified:
  - src/qa/checkpoint.py
  - src/qa/__init__.py
  - generate_qa.py
  - pipeline.py
autonomous: true

must_haves:
  truths:
    - "QA generation can resume after interruption without re-processing completed work"
    - "pipeline.py --generate-qa flag triggers full QA pipeline"
    - "Checkpoint state persists between runs"
  artifacts:
    - path: "src/qa/checkpoint.py"
      provides: "Checkpoint save/load/detect logic"
      exports: ["Checkpoint", "save_checkpoint", "load_checkpoint", "compute_file_hash"]
      min_lines: 40
    - path: "qa/.checkpoint.json"
      provides: "Checkpoint state file"
      format: "JSON with stage, progress, hashes"
  key_links:
    - from: "generate_qa.py"
      to: "src/qa/checkpoint.py"
      via: "import checkpoint functions"
      pattern: "from src.qa import.*checkpoint"
    - from: "pipeline.py"
      to: "generate_qa.py"
      via: "subprocess call"
      pattern: 'python.*generate_qa'
---

<objective>
Add checkpointing for resumable QA generation and integrate into main pipeline.

Purpose: Enable long-running QA generation to resume after interruption (INTG-03) and allow pipeline.py to optionally include QA generation as a stage (INTG-02). This completes the v5.0 QA Pipeline milestone.

Output:
- src/qa/checkpoint.py module with checkpoint logic
- Checkpointing integrated into generate_qa.py workflow
- pipeline.py --generate-qa flag for end-to-end QA generation
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/31-export-integration/31-RESEARCH.md
@.planning/phases/31-export-integration/31-CONTEXT.md
@.planning/phases/31-export-integration/31-01-SUMMARY.md

# Existing code
@generate_qa.py
@pipeline.py
@src/qa/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create checkpoint module and integrate into generate_qa.py</name>
  <files>src/qa/checkpoint.py, src/qa/__init__.py, generate_qa.py</files>
  <action>
Create src/qa/checkpoint.py with file-level checkpointing:

1. Pydantic model for checkpoint state:
   ```python
   class Checkpoint(BaseModel):
       stage: Literal["questions", "index", "answers", "validate", "export"]
       input_hash: str  # MD5 of input directory state
       completed_stages: list[str]  # Stages already done
       timestamp: str  # ISO format
   ```

2. Helper functions:
   ```python
   def compute_file_hash(path: Path) -> str:
       """Compute MD5 hash of file for change detection."""

   def compute_dir_hash(dir_path: Path, pattern: str = "*.md") -> str:
       """Compute hash of directory contents (sorted file list + sizes)."""

   def save_checkpoint(checkpoint_path: Path, checkpoint: Checkpoint) -> None:
       """Save checkpoint atomically using temp file + rename."""

   def load_checkpoint(checkpoint_path: Path) -> Checkpoint | None:
       """Load checkpoint if exists and valid."""

   def should_skip_stage(checkpoint: Checkpoint | None, stage: str, input_hash: str) -> bool:
       """Check if stage can be skipped (already completed with same input)."""
   ```

3. Update src/qa/__init__.py with exports

4. Integrate into generate_qa.py main():
   - Add checkpoint loading at start of main()
   - Compute input hash from converted/ directory
   - Before each stage (questions, build-index, answers, validate, export):
     - Check should_skip_stage()
     - If skip: print "Skipping {stage} (already completed)" and continue
     - After completion: update checkpoint.completed_stages, save_checkpoint()
   - Add --no-resume flag to force fresh run (ignores checkpoint)
   - On successful completion of all stages: clean up checkpoint file

5. Add --no-resume argument:
   ```python
   parser.add_argument(
       "--no-resume",
       action="store_true",
       help="Ignore checkpoint and run fresh (don't resume)"
   )
   ```

IMPORTANT: File-level checkpointing is sufficient - tracks which stages completed, not individual files within stages. If input directory changes (hash differs), invalidate checkpoint.
  </action>
  <verify>
```bash
# Check module imports
python3 -c "from src.qa import Checkpoint, save_checkpoint, load_checkpoint; print('Checkpoint module OK')"

# Run partial pipeline to create checkpoint
python3 generate_qa.py --limit 5  # Creates questions
ls -la qa/.checkpoint.json

# Re-run should skip questions stage
python3 generate_qa.py --limit 5 2>&1 | grep -i "skip"
```
  </verify>
  <done>
- src/qa/checkpoint.py exists with Checkpoint model and helper functions
- __init__.py updated with checkpoint exports
- generate_qa.py saves/loads checkpoint between stages
- Resuming skips completed stages
- --no-resume flag forces fresh run
  </done>
</task>

<task type="auto">
  <name>Task 2: Add --generate-qa flag to pipeline.py</name>
  <files>pipeline.py</files>
  <action>
Add --generate-qa flag to pipeline.py for end-to-end QA generation:

1. Add argument to parse_args():
   ```python
   parser.add_argument(
       "--generate-qa",
       action="store_true",
       help="Run QA generation pipeline after document processing (requires GEMINI_API_KEY)"
   )
   ```

2. Add QA pipeline stages after STAGE 5 (Combine Prompts):
   ```python
   # ========== STAGE 6: QA Generation (optional) ==========
   if args.generate_qa:
       # Check prerequisites
       if not os.environ.get('GEMINI_API_KEY'):
           print("WARNING: GEMINI_API_KEY not set, skipping QA generation")
       else:
           # Determine converted dir (run_dir or root)
           qa_input = converted_dir if converted_dir.exists() else Path("converted")
           qa_output = run_dir / "qa" if run_dir else Path("qa")

           # Stage 6a: Build index
           cmd = [python_exe, "generate_qa.py", "--build-index",
                  "--input", str(qa_input), "--output", str(qa_output)]
           success, duration = run_stage("QA Index (build FAISS index)", cmd)
           stage_times["qa_index"] = duration
           if not success:
               print("WARNING: QA index stage failed, skipping remaining QA stages")
           else:
               # Stage 6b: Generate questions
               cmd = [python_exe, "generate_qa.py", "--input", str(qa_input),
                      "--output", str(qa_output)]
               success, duration = run_stage("QA Questions (generate questions)", cmd)
               stage_times["qa_questions"] = duration

               # Stage 6c: Generate answers
               if success:
                   cmd = [python_exe, "generate_qa.py", "--answers",
                          "--output", str(qa_output), "--index-dir", str(qa_output / "embeddings")]
                   success, duration = run_stage("QA Answers (generate answers)", cmd)
                   stage_times["qa_answers"] = duration

               # Stage 6d: Validate
               if success:
                   cmd = [python_exe, "generate_qa.py", "--validate",
                          "--input", str(qa_input), "--output", str(qa_output),
                          "--index-dir", str(qa_output / "embeddings")]
                   success, duration = run_stage("QA Validate (validate pairs)", cmd)
                   stage_times["qa_validate"] = duration

               # Stage 6e: Export
               if success:
                   cmd = [python_exe, "generate_qa.py", "--export",
                          "--output", str(qa_output)]
                   success, duration = run_stage("QA Export (HuggingFace JSONL)", cmd)
                   stage_times["qa_export"] = duration
   ```

3. Update summary output to show QA results:
   ```python
   # After existing output summary
   if args.generate_qa:
       qa_dir = run_dir / "qa" if run_dir else Path("qa")
       if (qa_dir / "qa_pairs.jsonl").exists():
           # Count lines
           with open(qa_dir / "qa_pairs.jsonl") as f:
               qa_count = sum(1 for _ in f)
           print(f"  {qa_dir}/qa_pairs.jsonl: {qa_count} QA pairs")
   ```

4. Add to docstring examples:
   ```
   # Full pipeline with QA generation
   python pipeline.py --generate-qa

   # Skip scrape, include QA
   python pipeline.py --skip-scrape --generate-qa
   ```

IMPORTANT: QA stages should continue on failure (not exit pipeline) since document processing is the primary output. Just warn and skip remaining QA stages.
  </action>
  <verify>
```bash
# Test CLI shows new flag
python3 pipeline.py --help | grep -A2 "generate-qa"

# Dry-run check (won't actually run without Chrome)
echo "Flag added successfully"
```
  </verify>
  <done>
- --generate-qa flag added to pipeline.py
- QA stages run after document processing when flag is set
- Stages: index -> questions -> answers -> validate -> export
- Graceful failure handling (warns but continues)
- Summary shows QA output counts
  </done>
</task>

</tasks>

<verification>
```bash
# 1. Verify checkpoint module
python3 -c "from src.qa import Checkpoint, save_checkpoint, load_checkpoint; print('OK')"

# 2. Test resume functionality
python3 generate_qa.py --limit 3  # Creates partial state
python3 generate_qa.py --limit 3 2>&1 | head -20  # Should show skipping

# 3. Test --no-resume
python3 generate_qa.py --limit 3 --no-resume 2>&1 | head -10  # Should NOT skip

# 4. Verify pipeline flag exists
python3 pipeline.py --help | grep "generate-qa"

# 5. Full integration test (if GEMINI_API_KEY set)
# python3 pipeline.py --skip-scrape --generate-qa
```
</verification>

<success_criteria>
- [ ] src/qa/checkpoint.py exists with Checkpoint model and helper functions
- [ ] generate_qa.py saves checkpoint after each stage
- [ ] generate_qa.py skips completed stages on resume (based on input hash)
- [ ] --no-resume flag forces fresh run
- [ ] pipeline.py --generate-qa flag triggers QA stages
- [ ] QA stages run in order: index -> questions -> answers -> validate -> export
- [ ] Pipeline continues even if QA stages fail (warns only)
</success_criteria>

<output>
After completion, create `.planning/phases/31-export-integration/31-02-SUMMARY.md`
</output>
